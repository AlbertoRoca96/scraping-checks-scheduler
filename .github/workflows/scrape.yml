# .github/workflows/scrape.yml
name: Scraping & Checks Scheduler

on:
  schedule:
    # Fast lane (offset every ~16 minutes)
    - cron: "8/16 * * * *"
    # Full scrape every 2 hours (UTC)
    - cron: "0 */2 * * *"
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  # -------------------------
  # Full matrix every 2 hours
  # -------------------------
  scrape_full:
    # Run on manual dispatch OR when the 2-hour schedule fired
    if: ${{ github.event_name != 'schedule' || github.event.schedule == '0 */2 * * *' }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    concurrency:
      group: full-${{ github.ref }}-${{ matrix.group }}
      cancel-in-progress: true
    strategy:
      fail-fast: false
      matrix:
        group: ["stocks", "default", "seo", "price", "compliance"]

    steps:
      - name: Debug schedule
        run: echo "event=${{ github.event_name }} schedule='${{ github.event.schedule }}'"

      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Seed previous site data (latest + timeseries)
        run: |
          mkdir -p data/{latest,timeseries}
          rsync -a docs/data/latest/     data/latest/     2>/dev/null || true
          rsync -a docs/data/timeseries/ data/timeseries/ 2>/dev/null || true

      - name: Set up Node (with npm cache)
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: |
            **/package-lock.json
            **/npm-shrinkwrap.json
            **/yarn.lock

      - name: Set up Node (no cache)
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies
        run: |
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            npm ci
          else
            npm install
          fi

      - name: Install Playwright (browsers + Linux deps)
        run: npx playwright install --with-deps chromium

      - name: Run scrapers (group = ${{ matrix.group }})
        env:
          GROUP: ${{ matrix.group }}
          FAIL_ON_ERROR: "0"
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          ALPHAVANTAGE_KEY: ${{ secrets.ALPHAVANTAGE_KEY }}
        run: npm run scrape

      - name: Upload data as artifact (group = ${{ matrix.group }})
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-${{ matrix.group }}
          path: data
          if-no-files-found: error
          retention-days: 7

  # -----------------------------------------
  # Fast lane: stocks only, every ~16 minutes
  # -----------------------------------------
  scrape_stocks_fast:
    # Run on scheduled events that are NOT the 2-hour full scrape.
    # This makes the job robust to future changes to the "fast" cron string.
    if: ${{ github.event_name == 'schedule' && github.event.schedule != '0 */2 * * *' }}
    runs-on: ubuntu-latest
    timeout-minutes: 20
    concurrency:
      group: stocks-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Debug schedule
        run: echo "event=${{ github.event_name }} schedule='${{ github.event.schedule }}'"

      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Seed previous site data (latest + timeseries)
        run: |
          mkdir -p data/{latest,timeseries}
          rsync -a docs/data/latest/     data/latest/     2>/dev/null || true
          rsync -a docs/data/timeseries/ data/timeseries/ 2>/dev/null || true

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies
        run: |
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            npm ci
          else
            npm install
          fi

      # Playwright is imported at top-level, so install it here too
      - name: Install Playwright (browsers + Linux deps)
        run: npx playwright install --with-deps chromium

      - name: Run scrapers (stocks only)
        env:
          GROUP: stocks
          FAIL_ON_ERROR: "0"
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          ALPHAVANTAGE_KEY: ${{ secrets.ALPHAVANTAGE_KEY }}
        run: npm run scrape

      - name: Upload data as artifact (stocks)
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-stocks
          path: data
          if-no-files-found: error
          retention-days: 7

  # -----------------------------
  # Merge artifacts & publish web
  # -----------------------------
  merge_and_commit:
    needs: [scrape_full, scrape_stocks_fast]
    # Run if either lane succeeded (even if the other was skipped)
    if: ${{ always() && (needs.scrape_full.result == 'success' || needs.scrape_stocks_fast.result == 'success') }}
    runs-on: ubuntu-latest
    concurrency:
      group: publish-${{ github.ref }}
      cancel-in-progress: true

    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Seed working data from docs/data
        run: |
          mkdir -p data
          rsync -a docs/data/ data/ 2>/dev/null || true

      # Download each matrix artifact into its own folder
      - name: Download artifact (default)
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-default
          path: artifacts/default
        continue-on-error: true

      - name: Download artifact (seo)
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-seo
          path: artifacts/seo
        continue-on-error: true

      - name: Download artifact (price)
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-price
          path: artifacts/price
        continue-on-error: true

      - name: Download artifact (compliance)
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-compliance
          path: artifacts/compliance
        continue-on-error: true

      - name: Download artifact (stocks)
        uses: actions/download-artifact@v4
        with:
          name: scrape-results-stocks
          path: artifacts/stocks
        continue-on-error: true

      - name: Merge artifacts into working data and publish
        shell: bash
        run: |
          set -euxo pipefail
          merge_group() {
            local g="$1"
            if [ -d "artifacts/$g/data" ]; then
              rsync -a "artifacts/$g/data/" data/
            fi
            for sub in latest timeseries history reports; do
              if [ -d "artifacts/$g/$sub" ]; then
                rsync -a "artifacts/$g/$sub/" "data/$sub/"
              fi
            done
          }
          for d in default seo price compliance; do
            [ -d "artifacts/$d" ] && merge_group "$d"
          done
          [ -d "artifacts/stocks" ] && merge_group "stocks"

          mkdir -p docs/data
          rsync -a --delete data/ docs/data/

      - name: Commit updated data + site
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(site): publish data [skip ci]"
          file_pattern: "."
          add_options: "--all"
          skip_dirty_check: true
