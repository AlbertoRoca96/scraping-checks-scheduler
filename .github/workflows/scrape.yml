# Instead of one "download all" with merge-multiple:true,
# download each artifact into a separate dir
- name: Download artifact (stocks)
  uses: actions/download-artifact@v4
  with:
    name: scrape-results-stocks
    path: artifacts/stocks
  continue-on-error: true

- name: Download artifact (default)
  uses: actions/download-artifact@v4
  with:
    name: scrape-results-default
    path: artifacts/default
  continue-on-error: true

- name: Download artifact (seo)
  uses: actions/download-artifact@v4
  with:
    name: scrape-results-seo
    path: artifacts/seo
  continue-on-error: true

- name: Download artifact (price)
  uses: actions/download-artifact@v4
  with:
    name: scrape-results-price
    path: artifacts/price
  continue-on-error: true

- name: Download artifact (compliance)
  uses: actions/download-artifact@v4
  with:
    name: scrape-results-compliance
    path: artifacts/compliance
  continue-on-error: true

# Merge in a fixed order; put stocks LAST so its series.jsonl wins
- name: Merge artifacts into working data and publish
  shell: bash
  run: |
    set -euxo pipefail
    mkdir -p data
    rsync -a docs/data/ data/ 2>/dev/null || true

    for d in default seo price compliance; do
      [ -d "artifacts/$d/data" ] && rsync -a "artifacts/$d/data/" data/
    done
    [ -d "artifacts/stocks/data" ] && rsync -a "artifacts/stocks/data/" data/

    mkdir -p docs/data
    rsync -a --delete data/ docs/data/
