name: Scraping & Checks Scheduler

on:
  schedule:
    - cron: "15 */6 * * *"   # every 6h at :15 UTC
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape:
    strategy:
      fail-fast: false
      matrix:
        group: ["default", "seo", "price", "compliance"]
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      # Lockfile-aware setup + caching
      - name: Set up Node (with npm cache)
        if: ${{ hashFiles('package-lock.json') != '' || hashFiles('npm-shrinkwrap.json') != '' || hashFiles('yarn.lock') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Set up Node (no cache)
        if: ${{ hashFiles('package-lock.json') == '' && hashFiles('npm-shrinkwrap.json') == '' && hashFiles('yarn.lock') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies (npm ci)
        if: ${{ hashFiles('package-lock.json') != '' || hashFiles('npm-shrinkwrap.json') != '' }}
        run: npm ci

      - name: Install dependencies (npm install)
        if: ${{ hashFiles('package-lock.json') == '' && hashFiles('npm-shrinkwrap.json') == '' }}
        run: npm install

      - name: Install Playwright (browsers + Linux deps)
        run: npx playwright install --with-deps chromium

      - name: Run scrapers (group=${{ matrix.group }})
        env:
          GROUP: ${{ matrix.group }}
          FAIL_ON_ERROR: "0"             # set "1" to fail job on any check error
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}  # optional (Slack/Discord/etc.)
        run: npm run scrape

      - name: Upload data as artifact (group=${{ matrix.group }})
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-${{ matrix.group }}
          path: data

  merge_and_commit:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      # Download ALL group artifacts and MERGE their contents into CWD
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: scrape-results-*
          merge-multiple: true
          path: .

      # (Optional) Show what we got
      - name: Tree
        run: |
          echo "--- merged artifacts ---"
          ls -la
          echo "--- data dir ---"
          ls -la data || true

      # Single, conflict-free commit from merged artifacts
      - name: Auto-commit updated data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update scrape results [skip ci]"
          file_pattern: data/**
