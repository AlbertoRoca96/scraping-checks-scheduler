# .github/workflows/scrape.yml
name: Scraping & Checks Scheduler

on:
  schedule:
    - cron: "0 */2 * * *"   # every 2 hours
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape:
    strategy:
      fail-fast: false
      matrix:
        group: ["stocks", "default", "seo", "price", "compliance"]
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      # Use cache ONLY if a lockfile exists somewhere
      - name: Set up Node (with npm cache)
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') != '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: |
            **/package-lock.json
            **/npm-shrinkwrap.json
            **/yarn.lock

      - name: Set up Node (no cache)
        if: ${{ hashFiles('**/package-lock.json', '**/npm-shrinkwrap.json', '**/yarn.lock') == '' }}
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies
        run: |
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            npm ci
          else
            npm install
          fi

      - name: Install Playwright (browsers + Linux deps)
        run: npx playwright install --with-deps chromium

      - name: Run scrapers (group = ${{ matrix.group }})
        env:
          GROUP: ${{ matrix.group }}
          FAIL_ON_ERROR: "0"
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          ALPHAVANTAGE_KEY: ${{ secrets.ALPHAVANTAGE_KEY }}
        run: npm run scrape

      - name: Upload data as artifact (group = ${{ matrix.group }})
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-${{ matrix.group }}
          path: data
          if-no-files-found: error
          retention-days: 7

  merge_and_commit:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Download & merge all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: scrape-results-*
          merge-multiple: true
          path: .

      - name: Normalize + publish data (merge all groups)
        shell: bash
        run: |
          set -euxo pipefail

          # Ensure target dirs
          mkdir -p data/{latest,history,reports,timeseries}

          # Artifacts usually unpack under ./data/*
          if [ -d "./data" ]; then
            rsync -a ./data/latest/     data/latest/     || true
            rsync -a ./data/history/    data/history/    || true
            rsync -a ./data/reports/    data/reports/    || true
            rsync -a ./data/timeseries/ data/timeseries/ || true
          fi

          # Safety: if any group artifact unpacked flat (./latest, ./timeseries, â€¦), fold them in
          for d in latest history reports timeseries; do
            if [ -d "./$d" ]; then
              rsync -a "./$d/" "data/$d/"
              rm -rf "./$d"
            fi
          done

          # Move any top-level report-* files produced by scripts
          shopt -s nullglob
          for f in ./report-*.json ./report-*.md; do mv "$f" data/; done

          # Publish to GitHub Pages (docs/)
          mkdir -p docs/data
          rsync -a --delete data/ docs/data/

      - name: Commit updated data + site
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(site): publish data [skip ci]"
          file_pattern: "."
          add_options: "--all"
          skip_dirty_check: true
