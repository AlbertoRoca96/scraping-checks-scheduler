name: Scraping & Checks Scheduler

on:
  schedule:
    - cron: "15 */6 * * *"   # every 6h at :15 UTC
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape:
    strategy:
      fail-fast: false
      matrix:
        group: ["default", "seo", "price", "compliance"]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Set up Node (with cache if lockfile present)
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: ${{ hashFiles('package-lock.json','npm-shrinkwrap.json','yarn.lock') != '' && 'npm' || '' }}

      - name: Install dependencies
        run: |
          if [ -f package-lock.json ] || [ -f npm-shrinkwrap.json ]; then
            npm ci
          else
            npm install
          fi

      - name: Install Playwright (browsers + Linux deps)
        run: npx playwright install --with-deps chromium

      - name: Run scrapers (group=${{ matrix.group }})
        env:
          GROUP: ${{ matrix.group }}
          FAIL_ON_ERROR: "0"                   # set to "1" to fail on any check error
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
        run: npm run scrape

      - name: Upload data as artifact (group=${{ matrix.group }})
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-${{ matrix.group }}
          path: data
          if-no-files-found: error
          retention-days: 7

  merge_and_commit:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - name: Check out the repo
        uses: actions/checkout@v4

      - name: Download & merge all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: scrape-results-*
          merge-multiple: true
          path: .

      - name: Show what merged
        run: |
          echo "--- top level ---"; ls -la
          echo "--- data/ ---";    find data -maxdepth 2 -type f -printf "%P\n" | sort || true
          echo "--- latest/ ---";  find latest -maxdepth 1 -type f -printf "%P\n" | sort || true

      # Unify any stray top-level outputs into data/
      - name: Normalize artifact layout
        shell: bash
        run: |
          mkdir -p data/latest data/history data/reports
          # If previous runs created top-level latest/history/reports, fold them into data/
          [ -d latest   ] && rsync -a latest/   data/latest/   && rm -rf latest
          [ -d history  ] && rsync -a history/  data/history/  && rm -rf history
          [ -d reports  ] && rsync -a reports/  data/reports/  && rm -rf reports
          shopt -s nullglob
          for f in report-*.json report-*.md; do mv "$f" data/; done

      # Copy the unified data set to the site directory
      - name: Copy data to docs for Pages
        run: |
          mkdir -p docs/data
          rsync -a --delete data/ docs/data/

      - name: Preview docs/data
        run: |
          echo "--- docs/data ---"
          find docs/data -maxdepth 2 -type f -printf "%P\n" | sort | sed -n '1,200p'

      # Commit everything (including removals from normalization)
      - name: Commit updated data + site
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(site): normalize & publish data to docs [skip ci]"
          file_pattern: "."
          add_options: "--all"
          skip_dirty_check: true
